---
comments: true
---

# Transformer Models Optimization Techniques
I have been working on transformer models for a quite a while now. What I understand when I try to apply them for my problems is that:

- They are not easy to train / fine-tune
- They are difficult to explain
- They are difficult to deploy
- There are too many open source transformer models available right now and more and more are open sourced daily. So it's difficult to understand which is it the best one to use for the use case at hand.

So in this article I will try to explain how to optimize the training / fine-tuning of transformers using different optimization techniques people use. 

## Contens
- [Gradient Accumulation](#gradient-accumulation)
- [Automatic Mixed Precision](#automatic-mixed-precision)
- [Gradient Checkpointing](#gradient-checkpointing)
- [8-bit Optimizers](#8-bit-optimizers)
- [Fast Tokenizers](#fast-tokenizers)
- [Dynamic Padding](#dynamic-padding)
  - [Uniform Dynamic Padding](#uniform-dynamic-padding)

## Gradient Accumulation

## Automatic Mixed Precision

## Gradient Checkpointing

## 8-bit Optimizers

## Fast Tokenizers

## Dynamic Padding

### Uniform Dynamic Padding

## Freezing